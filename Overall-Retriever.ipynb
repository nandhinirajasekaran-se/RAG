{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac91544a",
   "metadata": {},
   "source": [
    "# Pdf Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "404d21b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k9/ywt32ng54d766jxqldqb4y7c0000gn/T/ipykernel_9448/3063426479.py:28: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceBgeEmbeddings(\n",
      "/opt/anaconda3/envs/clean-dspy/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: phi-standard.pdf\n",
      "Processing: phi-enhanced.pdf\n",
      "Processing: phi-basic.pdf\n",
      "\n",
      "Ingestion complete! Stored 248 chunks from 7 PDFs.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "# Configuration\n",
    "DOCS_DIR = \"basic/\"\n",
    "PERSIST_DIR = \"stores/insurance_metadata_v3\"\n",
    "EMBEDDING_MODEL = \"BAAI/bge-large-en\"\n",
    "\n",
    "def extract_plan_type(filename):\n",
    "    \"\"\"Extract plan type from filename (case-insensitive)\"\"\"\n",
    "    filename_lower = filename.lower()\n",
    "    if \"basic\" in filename_lower: return \"basic\"\n",
    "    if \"standard\" in filename_lower: return \"standard\"\n",
    "    if \"enhanced\" in filename_lower: return \"enhanced\"\n",
    "    return \"other\"\n",
    "\n",
    "# Initialize components\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True\n",
    ")\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL,\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": False}\n",
    ")\n",
    "\n",
    "# Process each PDF\n",
    "all_splits = []\n",
    "\n",
    "for pdf_file in os.listdir(DOCS_DIR):\n",
    "    if not pdf_file.endswith(\".pdf\"):\n",
    "        continue\n",
    "        \n",
    "    print(f\"Processing: {pdf_file}\")\n",
    "    file_path = os.path.join(DOCS_DIR, pdf_file)\n",
    "    \n",
    "    # Load PDF\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    pages = loader.load()\n",
    "    \n",
    "    # Split and add metadata\n",
    "    plan_type = extract_plan_type(pdf_file)\n",
    "    for page in pages:\n",
    "        splits = text_splitter.split_documents([page])\n",
    "        for split in splits:\n",
    "            split.metadata.update({\n",
    "                \"plan_type\": plan_type,\n",
    "                \"source_file\": pdf_file,\n",
    "                \"file_type\": \"pdf\"\n",
    "            })\n",
    "        all_splits.extend(splits)\n",
    "\n",
    "# Create vector store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=PERSIST_DIR\n",
    ")\n",
    "\n",
    "vectorstore._collection.modify(\n",
    "    metadata={\"allow_filtering\": True}  # Enable filtering by doc_type\n",
    ")\n",
    "\n",
    "print(f\"\\nIngestion complete! Stored {len(all_splits)} chunks from {len(os.listdir(DOCS_DIR))} PDFs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4bdcdd",
   "metadata": {},
   "source": [
    "# Extract data from Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72b446ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'phi-standard.pdf', 'page': 5, 'benefit': 'Drug'}, page_content='Drug coverage reimburses 70% on first $7,000\\n100% on next $93,000 with a maximum of $100,000 in a calendar year.'), Document(metadata={'source': 'phi-standard.pdf', 'page': 5, 'benefit': 'Extended health'}, page_content='Extended health coverage reimburses 100% with a maximum of Described in the Extended health\\nprovision section.'), Document(metadata={'source': 'phi-standard.pdf', 'page': 5, 'benefit': 'Vision'}, page_content='Vision coverage reimburses 100% with a maximum of $250 every two calendar years\\nAn insured person becomes\\neligible for vision coverage 1\\nyear after the effective date of\\nthis policy,.'), Document(metadata={'source': 'phi-standard.pdf', 'page': 5, 'benefit': 'Emergency travel medical\\ncoverage'}, page_content='Emergency travel medical\\ncoverage coverage reimburses 100% with a maximum of 60 days per trip\\n$1,000,000 lifetime.'), Document(metadata={'source': 'phi-standard.pdf', 'page': 5, 'benefit': 'Optional Benefits'}, page_content='Optional Benefits coverage reimburses None with a maximum of None.'), Document(metadata={'source': 'phi-standard.pdf', 'page': 5, 'benefit': 'Semi-private hospital\\nroom'}, page_content='Semi-private hospital\\nroom coverage reimburses 85% with a maximum of Described in the Semi-private\\nhospital room provision section.'), Document(metadata={'source': 'phi-standard.pdf', 'page': 5, 'benefit': 'Dental'}, page_content='Dental coverage reimburses 70% Preventive with a maximum of $750 in a calendar year\\nAn insured person becomes eligible\\nfor preventive dental coverage 3\\nmonths after the effective date of\\nthis policy..'), Document(metadata={'source': 'phi-enhanced.pdf', 'page': 5, 'benefit': 'Drug'}, page_content='Drug coverage reimburses 80% on first $5,000\\n100% on next $245,000 with a maximum of $250,000 in a calendar year.'), Document(metadata={'source': 'phi-enhanced.pdf', 'page': 5, 'benefit': 'Extended health'}, page_content='Extended health coverage reimburses 100% with a maximum of Described in the Extended health\\nprovision section.'), Document(metadata={'source': 'phi-enhanced.pdf', 'page': 5, 'benefit': 'Vision'}, page_content='Vision coverage reimburses 100% with a maximum of $300 every two calendar years\\nAn insured person becomes\\neligible for vision coverage 1\\nyear after the effective date of\\nthis policy..'), Document(metadata={'source': 'phi-enhanced.pdf', 'page': 5, 'benefit': 'Emergency travel medical\\ncoverage'}, page_content='Emergency travel medical\\ncoverage coverage reimburses 100% with a maximum of 60 days per trip\\n$1,000,000 lifetime.'), Document(metadata={'source': 'phi-enhanced.pdf', 'page': 5, 'benefit': 'Optional Benefits'}, page_content='Optional Benefits coverage reimburses None with a maximum of None.'), Document(metadata={'source': 'phi-enhanced.pdf', 'page': 5, 'benefit': 'Semi-private hospital\\nroom'}, page_content='Semi-private hospital\\nroom coverage reimburses 85% with a maximum of Described in the Semi-private\\nhospital room provision section.'), Document(metadata={'source': 'phi-enhanced.pdf', 'page': 5, 'benefit': 'Dental'}, page_content='Dental coverage reimburses 80% Preventive\\n50% Restorative\\n60% Orthodontic with a maximum of $750 in a calendar year\\nAn insured person becomes eligible\\nfor preventive dental coverage 3\\nmonths after the effective date of\\nthis policy\\n$500 in a calendar year\\nAn insured person becomes eligible\\nfor restorative dental coverage 1\\nyear after the effective date of this\\npolicy\\n$1,500 lifetime\\nAn insured person becomes eligible\\nfor orthodontic dental coverage 2\\nyears after the effective date of this\\npolicy.'), Document(metadata={'source': 'phi-basic.pdf', 'page': 5, 'benefit': 'Drug'}, page_content='Drug coverage reimburses 60% with a maximum of $750 in a calendar year.'), Document(metadata={'source': 'phi-basic.pdf', 'page': 5, 'benefit': 'Extended health'}, page_content='Extended health coverage reimburses 60% with a maximum of Described in the Extended health\\nprovision section.'), Document(metadata={'source': 'phi-basic.pdf', 'page': 5, 'benefit': 'Preventive dental'}, page_content='Preventive dental coverage reimburses 60% with a maximum of $500 in a calendar year\\nAn insured person becomes eligible\\nfor preventive dental coverage 3\\nmonths after the effective date of\\nthis policy.'), Document(metadata={'source': 'phi-basic.pdf', 'page': 5, 'benefit': 'Optional Benefits'}, page_content='Optional Benefits coverage reimburses None with a maximum of None.'), Document(metadata={'source': 'phi-basic.pdf', 'page': 5, 'benefit': 'Semi-private hospital\\nroom'}, page_content='Semi-private hospital\\nroom coverage reimburses 85% with a maximum of Described in the Semi-private\\nhospital room provision section.')]\n",
      "✅ 19 documents embedded and saved to vector store.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k9/ywt32ng54d766jxqldqb4y7c0000gn/T/ipykernel_9448/3533016856.py:46: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "\n",
    "def extract_structured_docs_from_pdf(pdf_path):\n",
    "    docs = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            tables = page.extract_tables()\n",
    "            for table in tables:\n",
    "                if table and len(table) > 1:  # Skip empty or header-only tables\n",
    "                    for row in table[1:]:  # Skip header row\n",
    "                        try:\n",
    "                            benefit, reimbursement, maximum = row\n",
    "                            sentence = f\"{benefit} coverage reimburses {reimbursement} with a maximum of {maximum}.\"\n",
    "                            metadata = {\n",
    "                                \"source\": os.path.basename(pdf_path),\n",
    "                                \"page\": i + 1,\n",
    "                                \"benefit\": benefit.strip()\n",
    "                            }\n",
    "                            docs.append(Document(page_content=sentence, metadata=metadata))\n",
    "                        except ValueError:\n",
    "                            continue  # skip malformed rows\n",
    "    return docs\n",
    "\n",
    "def process_pdf_folder(folder_path):\n",
    "    all_docs = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(folder_path, filename)\n",
    "            docs = extract_structured_docs_from_pdf(pdf_path)\n",
    "            all_docs.extend(docs)\n",
    "    return all_docs\n",
    "\n",
    "# Extract documents\n",
    "docs = process_pdf_folder(\"basic/\")\n",
    "print(docs)\n",
    "\n",
    "# Add new documents\n",
    "vectorstore.add_documents(docs)\n",
    "\n",
    "# Save updates\n",
    "vectorstore.persist()\n",
    "\n",
    "print(f\"✅ {len(docs)} documents embedded and saved to vector store.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f2f864",
   "metadata": {},
   "source": [
    "# WebPageLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af404929",
   "metadata": {},
   "outputs": [],
   "source": [
    "URLS = [\n",
    "    \"https://www.ontario.ca/page/what-ohip-covers\",\n",
    "    \"https://www.ontario.ca/page/ohip-coverage-while-outside-canada\",\n",
    "    \"https://www.ontario.ca/page/documents-needed-get-health-card\",\n",
    "    \"https://www.ontario.ca/page/military-families-services-and-support\",\n",
    "    \"https://www.ontario.ca/page/apply-ohip-and-get-health-card\",\n",
    "    \"https://uhip.ca/help-faq/\",\n",
    "    \"https://www.ontario.ca/page/learn-about-ohip-plus\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324c4cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🕸️ Loading webpages...\n",
      "✂️ Splitting documents...\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# 1. Define the custom text splitter\n",
    "class GovernmentTextSplitter(RecursiveCharacterTextSplitter):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            chunk_size=512,\n",
    "            chunk_overlap=64,\n",
    "            separators=[\n",
    "                \"\\n\\n\", \n",
    "                \"\\n\", \n",
    "                r\"(?<=\\. )\",  # Split after periods\n",
    "                \" \", \n",
    "                \"\"\n",
    "            ],\n",
    "            keep_separator=True\n",
    "        )\n",
    "\n",
    "# 2. Custom web loader with cleaning\n",
    "class GovernmentWebLoader(WebBaseLoader):\n",
    "    def __init__(self, urls):\n",
    "        super().__init__(urls)\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'\n",
    "        })\n",
    "    \n",
    "    def load(self) -> List[Document]:\n",
    "        docs = super().load()\n",
    "        return self.clean_documents(docs)\n",
    "    \n",
    "    def clean_documents(self, docs: List[Document]) -> List[Document]:\n",
    "        cleaned_docs = []\n",
    "        for doc in docs:\n",
    "            try:\n",
    "                soup = BeautifulSoup(doc.page_content, 'html.parser')\n",
    "                \n",
    "                # Remove unwanted elements\n",
    "                for element in soup(['script', 'style', 'nav', 'footer']):\n",
    "                    element.decompose()\n",
    "                \n",
    "                # Get clean text\n",
    "                text = soup.get_text('\\n', strip=True)\n",
    "                text = re.sub(r'\\n{3,}', '\\n\\n', text).strip()\n",
    "                \n",
    "                # Preserve important metadata\n",
    "                metadata = doc.metadata.copy()\n",
    "                metadata.update({\n",
    "                    \"doc_type\": \"OHIP\" if \"ontario.ca\" in doc.metadata[\"source\"] else \"UHIP\",\n",
    "                    \"cleaned\": True\n",
    "                })\n",
    "                \n",
    "                cleaned_docs.append(Document(\n",
    "                    page_content=text,\n",
    "                    metadata=metadata\n",
    "                ))\n",
    "            except Exception as e:\n",
    "                print(f\"Error cleaning document: {str(e)}\")\n",
    "                cleaned_docs.append(doc)\n",
    "        return cleaned_docs\n",
    "\n",
    "# 3. Vectorization pipeline\n",
    "def vectorize_webpages():\n",
    "    \n",
    "    print(\"🕸️ Loading webpages...\")\n",
    "    loader = GovernmentWebLoader(URLS)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    print(\"✂️ Splitting documents...\")\n",
    "    splitter = GovernmentTextSplitter()\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Run the pipeline\n",
    "chunks_web_based = vectorize_webpages()\n",
    "vectorstore.add_documents(chunks_web_based)\n",
    "\n",
    "# Save updates\n",
    "vectorstore.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41a5a070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing https://www.ontario.ca/page/what-ohip-covers...\n",
      "Processing https://www.ontario.ca/page/ohip-coverage-while-outside-canada...\n",
      "Processing https://www.ontario.ca/page/documents-needed-get-health-card...\n",
      "Processing https://www.ontario.ca/page/apply-ohip-and-get-health-card...\n",
      "Processing https://uhip.ca/help-faq/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k9/ywt32ng54d766jxqldqb4y7c0000gn/T/ipykernel_9448/2397432150.py:48: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n",
      "/var/folders/k9/ywt32ng54d766jxqldqb4y7c0000gn/T/ipykernel_9448/2397432150.py:48: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n",
      "/var/folders/k9/ywt32ng54d766jxqldqb4y7c0000gn/T/ipykernel_9448/2397432150.py:48: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n",
      "/var/folders/k9/ywt32ng54d766jxqldqb4y7c0000gn/T/ipykernel_9448/2397432150.py:48: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing https://uhip.ca/coverage-details/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k9/ywt32ng54d766jxqldqb4y7c0000gn/T/ipykernel_9448/2397432150.py:48: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n",
      "/var/folders/k9/ywt32ng54d766jxqldqb4y7c0000gn/T/ipykernel_9448/2397432150.py:48: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n",
      "/var/folders/k9/ywt32ng54d766jxqldqb4y7c0000gn/T/ipykernel_9448/2397432150.py:48: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n",
      "/var/folders/k9/ywt32ng54d766jxqldqb4y7c0000gn/T/ipykernel_9448/2397432150.py:48: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from langchain_core.documents import Document\n",
    "from urllib.parse import urljoin\n",
    "from typing import List\n",
    "\n",
    "# List of official OHIP/UHIP URLs\n",
    "OHIP_URLS = [\n",
    "    \"https://www.ontario.ca/page/what-ohip-covers\",\n",
    "    \"https://www.ontario.ca/page/ohip-coverage-while-outside-canada\",\n",
    "    \"https://www.ontario.ca/page/documents-needed-get-health-card\",\n",
    "    \"https://www.ontario.ca/page/apply-ohip-and-get-health-card\"\n",
    "]\n",
    "\n",
    "UHIP_URLS = [\n",
    "    \"https://uhip.ca/help-faq/\",\n",
    "    \"https://uhip.ca/coverage-details/\"\n",
    "]\n",
    "\n",
    "def fetch_with_retry(url: str, max_retries: int = 3) -> requests.Response:\n",
    "    \"\"\"Handle request failures with retries\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                url,\n",
    "                headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'},\n",
    "                timeout=10\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            time.sleep(2 ** attempt)  # Exponential backoff\n",
    "\n",
    "def extract_tables_from_url(url: str) -> List[Document]:\n",
    "    \"\"\"Extract all tables from a single URL\"\"\"\n",
    "    try:\n",
    "        response = fetch_with_retry(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        documents = []\n",
    "        \n",
    "        for i, table in enumerate(soup.find_all('table'), 1):\n",
    "            try:\n",
    "                # Extract table data\n",
    "                df = pd.read_html(str(table))[0]\n",
    "                \n",
    "                # Create document with context\n",
    "                table_text = f\"HEALTH TABLE FROM {url}:\\n{df.to_markdown(index=False)}\"\n",
    "                \n",
    "                documents.append(Document(\n",
    "                    page_content=table_text,\n",
    "                    metadata={\n",
    "                        \"source\": url + f\"#table_{i}\",\n",
    "                        \"doc_type\": \"OHIP\" if \"ontario.ca\" in url else \"UHIP\",\n",
    "                        \"columns\": str(list(df.columns)),\n",
    "                        \"row_count\": len(df),\n",
    "                        \"last_updated\": response.headers.get('Last-Modified', '')\n",
    "                    }\n",
    "                ))\n",
    "            except Exception as e:\n",
    "                print(f\"Skipped table {i} at {url}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {url}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def scrape_all_tables(url_list: List[str]) -> List[Document]:\n",
    "    \"\"\"Process multiple URLs in sequence\"\"\"\n",
    "    all_tables = []\n",
    "    for url in url_list:\n",
    "        print(f\"Processing {url}...\")\n",
    "        all_tables.extend(extract_tables_from_url(url))\n",
    "        time.sleep(1)  # Respectful delay between requests\n",
    "    return all_tables\n",
    "\n",
    "# Usage\n",
    "ohip_tables = scrape_all_tables(OHIP_URLS)\n",
    "uhip_tables = scrape_all_tables(UHIP_URLS)\n",
    "all_tables = ohip_tables + uhip_tables\n",
    "\n",
    "vectorstore.add_documents(all_tables)\n",
    "\n",
    "# 5. Save updates\n",
    "vectorstore.persist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93df2ddc",
   "metadata": {},
   "source": [
    "# OCR Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e9a21fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pytesseract\n",
    "from io import BytesIO\n",
    "import requests\n",
    "\n",
    "class OCRWebLoader(GovernmentWebLoader):\n",
    "    def __init__(self, urls):\n",
    "        super().__init__(urls)\n",
    "        pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'  # Update path as needed\n",
    "\n",
    "    def _extract_text_from_image(self, img_url: str) -> str:\n",
    "        try:\n",
    "            response = requests.get(img_url, stream=True)\n",
    "            img = Image.open(BytesIO(response.content))\n",
    "            return pytesseract.image_to_string(img)\n",
    "        except Exception as e:\n",
    "            print(f\"OCR failed for {img_url}: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        docs = super().load()\n",
    "        \n",
    "        for doc in docs:\n",
    "            soup = BeautifulSoup(doc.page_content, 'html.parser')\n",
    "            for img in soup.find_all('img', src=True):\n",
    "                if any(keyword in img['src'] for keyword in ['coverage', 'eligibility', 'table']):\n",
    "                    ocr_text = self._extract_text_from_image(img['src'])\n",
    "                    if ocr_text:\n",
    "                        doc.page_content += f\"\\n[IMAGE TEXT]: {ocr_text}\"\n",
    "                        doc.metadata['ocr_extracted'] = True\n",
    "        \n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25243224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def extract_pdf_tables(pdf_url: str) -> List[Document]:\n",
    "    try:\n",
    "        response = requests.get(pdf_url)\n",
    "        docs = []\n",
    "        \n",
    "        with pdfplumber.open(BytesIO(response.content)) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                # Extract text\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    docs.append(Document(\n",
    "                        page_content=text,\n",
    "                        metadata={\n",
    "                            \"source\": pdf_url,\n",
    "                            \"page\": page.page_number,\n",
    "                            \"type\": \"pdf_text\"\n",
    "                        }\n",
    "                    ))\n",
    "                \n",
    "                # Extract tables\n",
    "                for table in page.extract_tables():\n",
    "                    docs.append(Document(\n",
    "                        page_content=str(table),\n",
    "                        metadata={\n",
    "                            \"source\": pdf_url,\n",
    "                            \"page\": page.page_number,\n",
    "                            \"type\": \"pdf_table\"\n",
    "                        }\n",
    "                    ))\n",
    "        \n",
    "        return docs\n",
    "    except Exception as e:\n",
    "        print(f\"PDF extraction failed: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "501d95d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🕸️ Loading webpages...\n",
      "✂️ Splitting documents...\n"
     ]
    }
   ],
   "source": [
    "def enhanced_vectorization():\n",
    "    # Standard text content\n",
    "    text_urls = URLS\n",
    "    \n",
    "    # PDF resources\n",
    "    pdf_urls = [\n",
    "        \n",
    "    ]\n",
    "    \n",
    "    print(\"📄 Processing text content...\")\n",
    "    text_docs = OCRWebLoader(text_urls).load()\n",
    "    \n",
    "    print(\"📑 Processing PDF content...\")\n",
    "    pdf_docs = []\n",
    "    for pdf_url in pdf_urls:\n",
    "        pdf_docs.extend(extract_pdf_tables(pdf_url))\n",
    "    \n",
    "    print(\"✂️ Chunking documents...\")\n",
    "    all_docs = text_docs + pdf_docs\n",
    "    chunks = GovernmentTextSplitter().split_documents(all_docs)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "chucks_ocr = vectorize_webpages()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31de8b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.add_documents(chucks_ocr)\n",
    "\n",
    "# 5. Save updates\n",
    "vectorstore.persist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean-dspy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
